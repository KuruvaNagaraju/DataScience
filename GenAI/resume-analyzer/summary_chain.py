"""
Module: summary_chain

This module contains the `run_summary` function which summarizes and extracts key information
from a resume using a vLLM API.
"""

import logging
from typing import Dict, Any

from llm_config import query_vllm,query_ollama

# ----------------------------------------
# Configure logging
# ----------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] - %(message)s",
    handlers=[logging.FileHandler("summary_chain.log"), logging.StreamHandler()]
)

def run_summary(context: str) -> Dict[str, Any]:
    """
    Extracts structured information from a resume by summarizing the key details.

    Args:
        context (str): The resume content as text, typically extracted from a PDF file.

    Returns:
        dict: A dictionary containing the structured summary response:
            - 'answer' (str): The response generated by the LLM summarizing the resume.
    """
    try:
        logging.info("Preparing the prompt for vLLM summarization.")

        # Define the prompt to extract structured resume information
        prompt = f"""
Use the context below to extract structured information from a resume.

Context:
{context}

Instructions:
You are a professional HR assistant. Extract the following details from the resume:

1. Full Name  
2. Email Address  
3. Phone Number  
4. LinkedIn / GitHub / Portfolio URLs (if available)  
5. Current Job Title  
6. Current Company  
7. Total Years of Experience  
8. Education Details (Degrees, Institutions, Years)  
9. Certifications (if any)  
10. Key Technical Skills  
11. Soft Skills  
12. Professional Summary or Career Objective  
13. Notable Projects / Achievements  
14. Previous Companies with Roles and Duration  
15. Location (City, State, Country)

Provide the output in clean Markdown bullet points. Be concise and accurate. If any section is not found, mention it as “Not specified”.
"""

        # logging.info("Sending prompt to vLLM for summarization.")
        # llm_response = query_vllm(prompt)


        logging.info("Sending prompt to OLLAMA for summarization.")
        llm_response = query_ollama(prompt)

        # Return the LLM's summarized response
        return {
            "answer": llm_response
        }

    except Exception as e:
        logging.exception(f"Summary extraction failed: {e}")
        return {
            "answer": "[ERROR]: Unable to extract summary. Please try again."
        }
