"""
Module: rag_chain

This module provides a function `run_rag` to perform Retrieval-Augmented Generation (RAG)
on resume documents using a vector database and a local LLM (Ollama) with re-ranking using
a cross-encoder model from sentence-transformers.
"""

import logging
import os
from typing import Dict, Any, List
from dotenv import load_dotenv
from sentence_transformers import CrossEncoder

from vectorstore import load_vector_db
from llm_config import query_vllm,query_ollama

# ----------------------------------------
# Load environment variables
# ----------------------------------------
load_dotenv()
RE_RANKER_MODEL = os.getenv("RE_RANKER_MODEL", "cross-encoder/ms-marco-MiniLM-L12-v2")

# ----------------------------------------
# Configure logging
# ----------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] - %(message)s",
    handlers=[logging.FileHandler("rag_chain.log"), logging.StreamHandler()]
)

# ----------------------------------------
# Load Cross-Encoder model for re-ranking
# ----------------------------------------
logging.info(f"Loading cross-encoder model '{RE_RANKER_MODEL}' for re-ranking...")
cross_encoder = CrossEncoder(RE_RANKER_MODEL)


def rerank_documents(query: str, docs: List) -> List:
    """
    Re-rank retrieved documents using a cross-encoder for better relevance scoring.

    Args:
        query (str): User's input query.
        docs (List[Document]): Documents retrieved from the vector store.

    Returns:
        List[Document]: Top re-ranked documents.
    """
    if not docs:
        return []

    pairs = [(query, doc.page_content) for doc in docs]
    scores = cross_encoder.predict(pairs)
    scored_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)

    top_docs = [doc for _, doc in scored_docs[:4]]
    logging.info(f"Top re-ranked documents selected: {len(top_docs)}")
    return top_docs


def run_rag(query: str) -> Dict[str, Any]:
    """
    Executes a RAG (Retrieval-Augmented Generation) pipeline to answer a question based on a resume.

    Args:
        query (str): The user's question about the resume content.

    Returns:
        dict: A dictionary containing:
            - 'answer' (str): The response generated by the LLM.
            - 'sources' (List[Document]): The documents retrieved as context.
    """
    try:
        logging.info("Initializing vector DB retriever...")
        vectordb = load_vector_db()

        retriever = vectordb.as_retriever(search_kwargs={"k": 8})
        docs = retriever.get_relevant_documents(query)
        logging.info(f"Retrieved {len(docs)} relevant documents before re-ranking.")

        top_docs = rerank_documents(query, docs)

        context = "\n\n".join([doc.page_content for doc in top_docs])

        prompt = f"""
You are a professional HR assistant helping to analyze candidate resumes.

Use the provided resume text to answer the question in a clear, concise, and structured manner.

Context:
{context}

Question:
{query}

Answer:
"""

        # logging.info("Sending prompt to VLLM for inference.")
        # llm_response = query_vllm(prompt)

        logging.info("Sending prompt to Ollama for inference.")
        llm_response = query_ollama(prompt)

        return {
            "answer": llm_response,
            "sources": top_docs
        }

    except Exception as e:
        logging.exception(f"RAG execution failed: {e}")
        return {
            "answer": "[ERROR]: Unable to generate a response. Please try again.",
            "sources": []
        }
